{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Library Imports ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Optional: For visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 2. Global Hyperparameters ---\n",
    "# Model Dimensions (Keep these small for the sprint)\n",
    "D_MODEL = 64        # Embedding dimension\n",
    "NUM_HEADS = 4       # Number of attention heads\n",
    "NUM_LAYERS = 1      # Use 1 Encoder and 1 Decoder layer for simplicity\n",
    "D_FF = 128          # Feed-Forward hidden dimension\n",
    "\n",
    "# Training Parameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 50 \n",
    "\n",
    "# Sequence Parameters\n",
    "MAX_SEQ_LEN = 10    # Max tokens in any sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bedb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Vocabulary Definition and Mapping ---\n",
    "# The fixed, small vocabulary for our lookup table\n",
    "VOCAB = {\n",
    "    '[PAD]': 0, \n",
    "    '[SOS]': 1, \n",
    "    '[EOS]': 2,\n",
    "    \n",
    "    # Keys\n",
    "    'COLOR': 3, 'NAME': 4, 'ITEM': 5, \n",
    "    \n",
    "    # Values\n",
    "    'RED': 6, 'BLUE': 7, 'GREEN': 8,\n",
    "    'ALICE': 9, 'BOB': 10, 'CHAIR': 11,\n",
    "    \n",
    "    # Query Tokens\n",
    "    'QUERY': 12, 'FIND': 13\n",
    "}\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "ID_TO_TOKEN = {v: k for k, v in VOCAB.items()}\n",
    "\n",
    "# --- 2. The Lookup Dictionary (Ground Truth) ---\n",
    "# Defines all the possible fact pairings the model must learn.\n",
    "FACTS_DICT = {\n",
    "    # Key Token ID: Value Token ID\n",
    "    VOCAB['RED']: VOCAB['BLUE'],    # Red item's value is 'BLUE'\n",
    "    VOCAB['ALICE']: VOCAB['GREEN'], # Alice's value is 'GREEN'\n",
    "    VOCAB['CHAIR']: VOCAB['RED']    # Chair's value is 'RED'\n",
    "}\n",
    "FACT_PAIRS = list(FACTS_DICT.items()) # List of (Key_ID, Value_ID)\n",
    "\n",
    "# --- 3. Synthetic Data Generation Function (FIXED) ---\n",
    "def generate_synthetic_data(num_samples):\n",
    "    data = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # 1. Randomly select facts for the Encoder input (The Dictionary)\n",
    "        # Select 2 distinct facts to ensure context is required.\n",
    "        fact1_key, fact1_val = FACT_PAIRS[np.random.randint(len(FACT_PAIRS))]\n",
    "        \n",
    "        # 2. Randomly select one fact to be the target (The Query)\n",
    "        query_key_id = fact1_key\n",
    "        target_val_id = fact1_val\n",
    "        \n",
    "        # Construct Encoder Input (X): The facts (order is randomized later to test no-PE logic)\n",
    "        enc_input_tokens = [VOCAB['COLOR'], query_key_id, VOCAB['ITEM'], fact1_val] \n",
    "        np.random.shuffle(enc_input_tokens) # Randomize order: crucial for no-PE test!\n",
    "        \n",
    "        # Add a final token to indicate end of fact list\n",
    "        enc_input_tokens = [VOCAB['NAME']] + enc_input_tokens \n",
    "        \n",
    "        # Construct Decoder Target (Y): The value we want\n",
    "        # The target sequence is [Value, EOS]\n",
    "        # --- FIX APPLIED HERE: Used '[EOS]' instead of 'EOS' ---\n",
    "        dec_target_tokens = [target_val_id, VOCAB['[EOS]']]\n",
    "        \n",
    "        data.append({\n",
    "            'enc_input': torch.tensor(enc_input_tokens, dtype=torch.long),\n",
    "            'dec_target': torch.tensor(dec_target_tokens, dtype=torch.long),\n",
    "            'query_val_id': target_val_id # Used for checking/visualization\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# --- 4. PyTorch Dataset and DataLoader Setup (CORRECTED) ---\n",
    "class LookupDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # We need to return encoder input, decoder input (starts with SOS), and true target\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Decoder input is the target sequence shifted right (starts with [SOS])\n",
    "        # FIX: Accessing the token as '[SOS]' instead of 'SOS'\n",
    "        dec_input = torch.cat([torch.tensor([VOCAB['[SOS]']]), item['dec_target'][:-1]])\n",
    "        \n",
    "        # The true target is the original target, which excludes the [SOS] token\n",
    "        true_target = item['dec_target']\n",
    "        \n",
    "        return item['enc_input'], dec_input, true_target\n",
    "\n",
    "# Collate function to handle padding for the DataLoader\n",
    "def collate_fn(batch):\n",
    "    # Padding sequences to MAX_SEQ_LEN\n",
    "    enc_inputs = [item[0] for item in batch]\n",
    "    dec_inputs = [item[1] for item in batch]\n",
    "    dec_targets = [item[2] for item in batch]\n",
    "    \n",
    "    # Use torch.nn.utils.rnn.pad_sequence for consistent padding\n",
    "    enc_inputs = nn.utils.rnn.pad_sequence(enc_inputs, batch_first=True, padding_value=VOCAB['[PAD]'])\n",
    "    dec_inputs = nn.utils.rnn.pad_sequence(dec_inputs, batch_first=True, padding_value=VOCAB['[PAD]'])\n",
    "    dec_targets = nn.utils.rnn.pad_sequence(dec_targets, batch_first=True, padding_value=VOCAB['[PAD]'])\n",
    "    \n",
    "    # Create padding masks (True means mask, False means keep)\n",
    "    src_padding_mask = (enc_inputs == VOCAB['[PAD]'])\n",
    "    tgt_padding_mask = (dec_inputs == VOCAB['[PAD]'])\n",
    "    \n",
    "    return enc_inputs, dec_inputs, dec_targets, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "# Generate and split data\n",
    "full_data = generate_synthetic_data(num_samples=5000)\n",
    "train_split = int(0.8 * 5000)\n",
    "val_split = int(0.1 * 5000)\n",
    "\n",
    "train_data = full_data[:train_split]\n",
    "val_data = full_data[train_split:train_split + val_split]\n",
    "test_data = full_data[train_split + val_split:]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(LookupDataset(train_data), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(LookupDataset(val_data), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(LookupDataset(test_data), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "print(f\"Data ready: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Scaled Dot-Product Attention Function (Helper) ---\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Calculates the scaled dot-product attention.\n",
    "    Q, K, V: [batch_size, n_heads, seq_len, d_k]\n",
    "    \"\"\"\n",
    "    d_k = Q.size(-1)\n",
    "    # [B, H, Lq, Lk]\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "    if mask is not None:\n",
    "        # Mask is applied before softmax. Masked positions are set to a very large negative number.\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    # [B, H, Lq, Lk]\n",
    "    attention_weights = torch.softmax(scores, dim=-1)\n",
    "    \n",
    "    # [B, H, Lq, Lk] @ [B, H, Lk, Dv] -> [B, H, Lq, Dv]\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# --- 2. Multi-Head Attention (MHA) Class ---\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads # Dimension of K, Q, V for a single head\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        # Linear layers for Q, K, V and the final output\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.size(0)\n",
    "        \n",
    "        # 1. Linear projections\n",
    "        # [B, L, D] -> [B, L, D]\n",
    "        q = self.W_q(Q)\n",
    "        k = self.W_k(K)\n",
    "        v = self.W_v(V)\n",
    "        \n",
    "        # 2. Split into heads\n",
    "        # [B, L, D] -> [B, L, H, d_k] -> [B, H, L, d_k]\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = k.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = v.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 3. Calculate attention\n",
    "        # x: [B, H, Lq, d_k], attn: [B, H, Lq, Lk]\n",
    "        x, self.attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        \n",
    "        # 4. Concatenate heads\n",
    "        # [B, H, Lq, d_k] -> [B, Lq, H, d_k] -> [B, Lq, D]\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 5. Final linear layer\n",
    "        # [B, Lq, D] -> [B, Lq, D]\n",
    "        output = self.W_o(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# --- 3. Feed-Forward Network (FFN) Class ---\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [B, L, D_model] -> [B, L, D_ff]\n",
    "        x = self.relu(self.linear_1(x))\n",
    "        # [B, L, D_ff] -> [B, L, D_model]\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "# Helper for Add&Norm (Residual Connection + Layer Norm)\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # LayerNorm(x + sublayer(x))\n",
    "        return x + sublayer(self.norm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4790a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. The Encoder Layer ---\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        # Self-Attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.sublayer = nn.ModuleList([SublayerConnection(d_model) for _ in range(2)])\n",
    "        \n",
    "    def forward(self, x, src_mask):\n",
    "        # 1. Self-Attention Sublayer\n",
    "        # Input is Q=x, K=x, V=x\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask=src_mask))\n",
    "        \n",
    "        # 2. Feed-Forward Sublayer\n",
    "        x = self.sublayer[1](x, self.feed_forward)\n",
    "        return x\n",
    "\n",
    "# --- 2. The Decoder Layer ---\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        super().__init__()\n",
    "        # Masked Self-Attention\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        # Cross-Attention\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.sublayer = nn.ModuleList([SublayerConnection(d_model) for _ in range(3)])\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        # 1. Masked Self-Attention Sublayer\n",
    "        # Input is Q=x, K=x, V=x\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask=tgt_mask))\n",
    "        \n",
    "        # 2. Cross-Attention Sublayer (Q=x, K=memory, V=memory)\n",
    "        # 'memory' is the output of the Encoder\n",
    "        x = self.sublayer[1](x, lambda x: self.cross_attn(x, memory, memory, mask=src_mask))\n",
    "        \n",
    "        # 3. Feed-Forward Sublayer\n",
    "        x = self.sublayer[2](x, self.feed_forward)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper for Positional Encoding (Only for Decoder)\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # --- Full PE Matrix Calculation ---\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        \n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-(np.log(10000.0) / d_model))\n",
    "        )\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0) # Shape: [1, max_len, d_model]\n",
    "        \n",
    "        # Register the buffer\n",
    "        self.register_buffer('pe', pe) \n",
    "\n",
    "    # CORRECT FORWARD METHOD: Only takes 'x' and adds the PE\n",
    "    def forward(self, x):\n",
    "        # x is [B, L, D_model]. Add the PE to the input embeddings.\n",
    "        # [B, L, D_model] + [1, L, D_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "# --- 1. Full Encoder Module ---\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # **CRITICAL**: No Positional Encoding added here for content-addressability test.\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        # [B, L] -> [B, L, D_model]\n",
    "        x = self.embedding(src)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            # Ensure src_mask is [B, 1, L_src] and ready for broadcast inside MHA\n",
    "            x = layer(x, src_mask)\n",
    "        \n",
    "        return self.norm(x)\n",
    "\n",
    "# --- 2. Full Decoder Module ---\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoding(d_model, max_len=max_seq_len) # PE is typically required for generation\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, memory, src_mask, tgt_mask):\n",
    "        # [B, L] -> [B, L, D_model]\n",
    "        x = self.embedding(tgt)\n",
    "        x = self.pe(x) # Add Positional Encoding\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "            \n",
    "        return self.norm(x)\n",
    "\n",
    "# --- 3. CrossAttentionLookupModel (The Final Model) ---\n",
    "class CrossAttentionLookupModel(nn.Module):\n",
    "    # FIX APPLIED HERE: Added all 6 required parameters to the __init__ signature\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len): \n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size, d_model, num_heads, d_ff, num_layers)\n",
    "        self.decoder = Decoder(vocab_size, d_model, num_heads, d_ff, num_layers, max_seq_len)\n",
    "        self.projection = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_padding_mask, tgt_padding_mask):\n",
    "        # 1. Prepare 4D Encoder Padding Mask\n",
    "        # FROM: [B, L_src] -> [B, 1, L_src] (Incorrect)\n",
    "        # TO:   [B, L_src] -> [B, 1, 1, L_src] (Correct for MHA broadcast)\n",
    "        # Convert boolean mask (True=PAD) to float mask (1=PAD) for MHA if needed, but for padded_fill, the boolean mask is fine.\n",
    "        # We need the 4D shape: [B, 1, 1, L_src]\n",
    "        encoder_src_mask = src_padding_mask.unsqueeze(1).unsqueeze(2) \n",
    "        \n",
    "        # Encoder Output: [B, L_src, D_model]\n",
    "        memory = self.encoder(src, encoder_src_mask) # Pass the 4D mask\n",
    "\n",
    "        # 2. Prepare 4D Decoder Self-Attention Mask (Causal + Padding)\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        tgt_causal_mask = (torch.triu(torch.ones(tgt_seq_len, tgt_seq_len, device=src.device), diagonal=1) == 1) # [L_tgt, L_tgt]\n",
    "        \n",
    "        # Combine causal and padding masks: [B, 1, L_tgt, L_tgt]\n",
    "        # full_tgt_mask = tgt_causal_mask.unsqueeze(0).unsqueeze(1) | tgt_padding_mask.unsqueeze(1).unsqueeze(2) # Original logic was overcomplicating the broadcast\n",
    "        \n",
    "        # A simple, robust way for Self-Attention Mask: [B, 1, L_tgt, L_tgt]\n",
    "        # The padding mask must be expanded to [B, 1, L_tgt, 1] to mask the queries, \n",
    "        # and [B, 1, 1, L_tgt] to mask the keys/values.\n",
    "        # Let's use the explicit 4D mask for the full self-attention weights.\n",
    "        \n",
    "        # Combine Causal Mask [L_tgt, L_tgt] and Padding Mask [B, L_tgt]\n",
    "        # Causal mask: [1, 1, L_tgt, L_tgt]. Padding mask (for query tokens): [B, 1, L_tgt, 1].\n",
    "        # Padding mask (for key tokens): [B, 1, 1, L_tgt].\n",
    "        \n",
    "        # Use simpler boolean mask logic for PyTorch MHA (which takes a 2D or 3D mask)\n",
    "        # For simplicity and to match the 4D input of scaled_dot_product_attention:\n",
    "        \n",
    "        full_tgt_mask = tgt_causal_mask.unsqueeze(0).unsqueeze(1) # [1, 1, L_tgt, L_tgt]\n",
    "        \n",
    "        # Padding Mask for Keys/Values: [B, 1, 1, L_tgt]\n",
    "        key_padding_mask = tgt_padding_mask.unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        # Add key padding to causal mask:\n",
    "        full_tgt_mask = full_tgt_mask | key_padding_mask # This results in [B, 1, L_tgt, L_tgt]\n",
    "\n",
    "        # 3. Prepare 4D Cross-Attention Padding Mask (Same as encoder_src_mask, just renamed)\n",
    "        final_src_mask_for_decoder = encoder_src_mask \n",
    "\n",
    "        decoder_output = self.decoder(\n",
    "            tgt, \n",
    "            memory, \n",
    "            # Cross-Attention K/V padding: [B, 1, 1, L_src]\n",
    "            final_src_mask_for_decoder,\n",
    "            # Decoder Self-Attention mask: [B, 1, L_tgt, L_tgt]\n",
    "            full_tgt_mask \n",
    "        )\n",
    "\n",
    "        # Final linear projection\n",
    "        output = self.projection(decoder_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c97519",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- 1. Initialization ---\n",
    "model = CrossAttentionLookupModel(\n",
    "    VOCAB_SIZE, D_MODEL, NUM_HEADS, D_FF, NUM_LAYERS, MAX_SEQ_LEN\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# Ignore [PAD] token in loss calculation\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=VOCAB['[PAD]'])\n",
    "\n",
    "# Variables to track best performance\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Training Loop ---\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for enc_input, dec_input, dec_target, src_padding_mask, tgt_padding_mask in train_loader:\n",
    "        enc_input, dec_input, dec_target = enc_input.to(device), dec_input.to(device), dec_target.to(device)\n",
    "        src_padding_mask, tgt_padding_mask = src_padding_mask.to(device), tgt_padding_mask.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: output is [B, L_tgt, VOCAB_SIZE]\n",
    "        output = model(enc_input, dec_input, src_padding_mask, tgt_padding_mask)\n",
    "        \n",
    "        # Reshape for CrossEntropyLoss: [B*L, VOCAB_SIZE] and [B*L]\n",
    "        loss = criterion(output.reshape(-1, VOCAB_SIZE), dec_target.reshape(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    \n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for enc_input, dec_input, dec_target, src_padding_mask, tgt_padding_mask in val_loader:\n",
    "            enc_input, dec_input, dec_target = enc_input.to(device), dec_input.to(device), dec_target.to(device)\n",
    "            src_padding_mask, tgt_padding_mask = src_padding_mask.to(device), tgt_padding_mask.to(device)\n",
    "\n",
    "            output = model(enc_input, dec_input, src_padding_mask, tgt_padding_mask)\n",
    "            loss = criterion(output.reshape(-1, VOCAB_SIZE), dec_target.reshape(-1))\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    \n",
    "    # Track and save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'best_lookup_model.pth')\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "print(\"--- Training Complete ---\")\n",
    "# Load the best model weights\n",
    "model.load_state_dict(torch.load('best_lookup_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Corrected Inference Function (Snippet) ---\n",
    "def greedy_decode(model, enc_input, max_len):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_padding_mask = (enc_input == VOCAB['[PAD]']).to(device)\n",
    "        \n",
    "        # 1. Encoder forward pass: Mask for Encoder Self-Attention [B, 1, 1, L_src]\n",
    "        memory = model.encoder(enc_input, src_padding_mask.unsqueeze(1).unsqueeze(2)) \n",
    "        \n",
    "        dec_input = torch.tensor([[VOCAB['[SOS]']]], dtype=torch.long, device=device)\n",
    "        \n",
    "        # 2. Cross-Attention Mask: Must match the Encoder Self-Attention shape\n",
    "        # [B, 1, 1, L_src]\n",
    "        final_src_mask_for_decoder = src_padding_mask.unsqueeze(1).unsqueeze(2) # Corrected!\n",
    "        \n",
    "        # ... (rest of the loop) ...\n",
    "        for _ in range(max_len):\n",
    "            tgt_len = dec_input.size(1)\n",
    "            \n",
    "            # 1. Causal mask definition (MISSING LINE)\n",
    "            tgt_causal_mask = (torch.triu(torch.ones(tgt_len, tgt_len), diagonal=1) == 1).to(device)\n",
    "            # ...\n",
    "            decoder_output = model.decoder(\n",
    "                dec_input, \n",
    "                memory, \n",
    "                # Use the correct 4D mask for K/V padding\n",
    "                final_src_mask_for_decoder, \n",
    "                # Causal mask: [1, 1, L_tgt, L_tgt]\n",
    "                tgt_causal_mask.unsqueeze(0).unsqueeze(1)\n",
    "            )\n",
    "            \n",
    "            # Project to vocab size: [1, L_tgt, VOCAB_SIZE]\n",
    "            output = model.projection(decoder_output[:, -1:]) # Only take the last token's output\n",
    "            \n",
    "            # Greedy search: get the token with highest probability\n",
    "            next_token = torch.argmax(output, dim=-1) # [1, 1]\n",
    "            \n",
    "            # Append to sequence\n",
    "            dec_input = torch.cat([dec_input, next_token], dim=1)\n",
    "            \n",
    "            # Stop condition\n",
    "            if next_token.item() == VOCAB['[EOS]']:\n",
    "                break\n",
    "                \n",
    "        # Return the generated sequence (excluding [SOS])\n",
    "        return dec_input[0, 1:]\n",
    "\n",
    "# --- 2. The Scrambling Test Execution ---\n",
    "print(\"\\n--- Scrambling Test (Content-Addressability) ---\")\n",
    "# Get one sample from the test set\n",
    "test_sample = next(iter(test_loader))\n",
    "enc_input_batch, _, dec_target_batch, _, _ = test_sample\n",
    "original_enc_input = enc_input_batch[0:1].to(device) # [1, L_src]\n",
    "true_target = [ID_TO_TOKEN[i.item()] for i in dec_target_batch[0] if i.item() != VOCAB['[PAD]']]\n",
    "\n",
    "# Original Order Inference\n",
    "original_output = greedy_decode(model, original_enc_input, max_len=MAX_SEQ_LEN)\n",
    "original_output_tokens = [ID_TO_TOKEN[i.item()] for i in original_output if i.item() != VOCAB['[EOS]']]\n",
    "\n",
    "# Scramble the Encoder Input (excluding the first token 'NAME' for a stable query)\n",
    "# and the padding tokens.\n",
    "enc_list = original_enc_input[0].cpu().numpy().tolist()\n",
    "non_pad_tokens = [t for t in enc_list if t != VOCAB['[PAD]']]\n",
    "first_token = non_pad_tokens[0]\n",
    "scramblable_tokens = non_pad_tokens[1:]\n",
    "\n",
    "np.random.shuffle(scramblable_tokens)\n",
    "scrambled_list = [first_token] + scramblable_tokens\n",
    "# Pad back to original length\n",
    "scrambled_list.extend([VOCAB['[PAD]']] * (original_enc_input.size(1) - len(scrambled_list)))\n",
    "\n",
    "scrambled_enc_input = torch.tensor([scrambled_list], dtype=torch.long, device=device)\n",
    "\n",
    "# Scrambled Order Inference\n",
    "scrambled_output = greedy_decode(model, scrambled_enc_input, max_len=MAX_SEQ_LEN)\n",
    "scrambled_output_tokens = [ID_TO_TOKEN[i.item()] for i in scrambled_output if i.item() != VOCAB['[EOS]']]\n",
    "\n",
    "print(f\"True Target: {true_target}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Print results\n",
    "print(f\"Original Input: {' '.join([ID_TO_TOKEN[i.item()] for i in original_enc_input[0] if i.item() != VOCAB['[PAD]']])}\")\n",
    "print(f\"Original Output: {' '.join(original_output_tokens)}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Scrambled Input: {' '.join([ID_TO_TOKEN[i.item()] for i in scrambled_enc_input[0] if i.item() != VOCAB['[PAD]']])}\")\n",
    "print(f\"Scrambled Output: {' '.join(scrambled_output_tokens)}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Outputs Identical: **{original_output_tokens == scrambled_output_tokens}** (Expected: True)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1dc164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Attention Hook Setup ---\n",
    "# The target layer is the cross_attn in the first (and only) DecoderLayer\n",
    "cross_attn_module = model.decoder.layers[0].cross_attn\n",
    "\n",
    "# A list to store the attention weights\n",
    "attention_weights_storage = []\n",
    "\n",
    "def capture_attention_hook(module, input, output):\n",
    "    # output[1] is the attention weights tensor from MultiHeadAttention\n",
    "    # Shape: [B, H, L_tgt, L_src]\n",
    "    attention_weights_storage.append(module.attention_weights.detach().cpu())\n",
    "\n",
    "# Register the forward hook\n",
    "hook = cross_attn_module.register_forward_hook(capture_attention_hook)\n",
    "\n",
    "# --- 2. Extraction Run ---\n",
    "# Run a single batch from the test_loader\n",
    "test_batch = next(iter(test_loader))\n",
    "enc_input, dec_input, dec_target, src_padding_mask, tgt_padding_mask = [t.to(device) for t in test_batch]\n",
    "\n",
    "# Run model in eval mode to trigger the hook\n",
    "with torch.no_grad():\n",
    "    _ = model(enc_input, dec_input, src_padding_mask, tgt_padding_mask)\n",
    "\n",
    "# Remove the hook after extraction\n",
    "hook.remove()\n",
    "\n",
    "# Extract the attention matrix for the first head, first batch item\n",
    "# [1, H, L_tgt, L_src] -> [L_tgt, L_src]\n",
    "attention_matrix = attention_weights_storage[0][0, 0].squeeze().numpy()\n",
    "\n",
    "# Get token labels for the plot\n",
    "# First sample in the batch\n",
    "enc_tokens_ids = enc_input[0].cpu().numpy().tolist()\n",
    "dec_tokens_ids = dec_input[0].cpu().numpy().tolist()\n",
    "\n",
    "# Filter out PAD tokens and map to actual tokens\n",
    "enc_tokens = [ID_TO_TOKEN[i] for i in enc_tokens_ids if i != VOCAB['[PAD]']]\n",
    "# dec_input has [SOS, Target_Val, EOS, PAD...]\n",
    "dec_tokens = [ID_TO_TOKEN[i] for i in dec_tokens_ids if i != VOCAB['[PAD]']]\n",
    "\n",
    "# Trim attention matrix to non-padded tokens\n",
    "L_tgt = len(dec_tokens)\n",
    "L_src = len(enc_tokens)\n",
    "attention_matrix = attention_matrix[:L_tgt, :L_src]\n",
    "\n",
    "\n",
    "# --- 3. Visualization ---\n",
    "def plot_cross_attention(attention_matrix, enc_tokens, dec_tokens):\n",
    "    \"\"\"Plots the cross-attention matrix as a heatmap.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Use Seaborn for a nicer heatmap\n",
    "    sns.heatmap(\n",
    "        attention_matrix, \n",
    "        xticklabels=enc_tokens, \n",
    "        yticklabels=dec_tokens, \n",
    "        cmap=\"Blues\", \n",
    "        linewidths=.5, \n",
    "        linecolor='black',\n",
    "        annot=True,\n",
    "        fmt=\".2f\",\n",
    "        cbar_kws={'label': 'Attention Weight'}\n",
    "    )\n",
    "    \n",
    "    plt.xlabel('Encoder Input Tokens (K/V)')\n",
    "    plt.ylabel('Decoder Input Tokens (Q)')\n",
    "    plt.title('Cross-Attention Heatmap (Query-to-Fact)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Execute plot_cross_attention to display the key heatmap.\n",
    "plot_cross_attention(attention_matrix, enc_tokens, dec_tokens)\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cross_attn_env (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
