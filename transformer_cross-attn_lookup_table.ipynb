{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Library Imports ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Optional: For visualization\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- 2. Global Hyperparameters ---\n",
    "# Model Dimensions (Keep these small for the sprint)\n",
    "D_MODEL = 64        # Embedding dimension\n",
    "NUM_HEADS = 4       # Number of attention heads\n",
    "NUM_LAYERS = 1      # Use 1 Encoder and 1 Decoder layer for simplicity\n",
    "D_FF = 128          # Feed-Forward hidden dimension\n",
    "\n",
    "# Training Parameters\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 50 \n",
    "\n",
    "# Sequence Parameters\n",
    "MAX_SEQ_LEN = 10    # Max tokens in any sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bedb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Vocabulary Definition and Mapping ---\n",
    "# The fixed, small vocabulary for our lookup table\n",
    "VOCAB = {\n",
    "    '[PAD]': 0, \n",
    "    '[SOS]': 1, \n",
    "    '[EOS]': 2,\n",
    "    \n",
    "    # Keys\n",
    "    'COLOR': 3, 'NAME': 4, 'ITEM': 5, \n",
    "    \n",
    "    # Values\n",
    "    'RED': 6, 'BLUE': 7, 'GREEN': 8,\n",
    "    'ALICE': 9, 'BOB': 10, 'CHAIR': 11,\n",
    "    \n",
    "    # Query Tokens\n",
    "    'QUERY': 12, 'FIND': 13\n",
    "}\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "ID_TO_TOKEN = {v: k for k, v in VOCAB.items()}\n",
    "\n",
    "# --- 2. The Lookup Dictionary (Ground Truth) ---\n",
    "# Defines all the possible fact pairings the model must learn.\n",
    "FACTS_DICT = {\n",
    "    # Key Token ID: Value Token ID\n",
    "    VOCAB['RED']: VOCAB['BLUE'],    # Red item's value is 'BLUE'\n",
    "    VOCAB['ALICE']: VOCAB['GREEN'], # Alice's value is 'GREEN'\n",
    "    VOCAB['CHAIR']: VOCAB['RED']    # Chair's value is 'RED'\n",
    "}\n",
    "FACT_PAIRS = list(FACTS_DICT.items()) # List of (Key_ID, Value_ID)\n",
    "\n",
    "# --- 3. Synthetic Data Generation Function ---\n",
    "def generate_synthetic_data(num_samples):\n",
    "    data = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # 1. Randomly select facts for the Encoder input (The Dictionary)\n",
    "        # Select 2 distinct facts to ensure context is required.\n",
    "        fact1_key, fact1_val = FACT_PAIRS[np.random.randint(len(FACT_PAIRS))]\n",
    "        \n",
    "        # 2. Randomly select one fact to be the target (The Query)\n",
    "        query_key_id = fact1_key\n",
    "        target_val_id = fact1_val\n",
    "        \n",
    "        # Construct Encoder Input (X): The facts (order is randomized later to test no-PE logic)\n",
    "        enc_input_tokens = [VOCAB['COLOR'], query_key_id, VOCAB['ITEM'], fact1_val] \n",
    "        np.random.shuffle(enc_input_tokens) # Randomize order: crucial for no-PE test!\n",
    "        \n",
    "        # Add a final token to indicate end of fact list\n",
    "        enc_input_tokens = [VOCAB['NAME']] + enc_input_tokens \n",
    "        \n",
    "        # Construct Decoder Target (Y): The value we want\n",
    "        # The target sequence is [Value, EOS]\n",
    "        dec_target_tokens = [target_val_id, VOCAB['EOS']]\n",
    "        \n",
    "        data.append({\n",
    "            'enc_input': torch.tensor(enc_input_tokens, dtype=torch.long),\n",
    "            'dec_target': torch.tensor(dec_target_tokens, dtype=torch.long),\n",
    "            'query_val_id': target_val_id # Used for checking/visualization\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "# --- 4. PyTorch Dataset and DataLoader Setup ---\n",
    "class LookupDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # We need to return encoder input, decoder input (starts with SOS), and true target\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Decoder input is the target sequence shifted right (starts with [SOS])\n",
    "        dec_input = torch.cat([torch.tensor([VOCAB['SOS']]), item['dec_target'][:-1]])\n",
    "        \n",
    "        # The true target is the original target, which excludes the [SOS] token\n",
    "        true_target = item['dec_target']\n",
    "        \n",
    "        return item['enc_input'], dec_input, true_target\n",
    "\n",
    "# Collate function to handle padding for the DataLoader\n",
    "def collate_fn(batch):\n",
    "    # Padding sequences to MAX_SEQ_LEN\n",
    "    enc_inputs = [item[0] for item in batch]\n",
    "    dec_inputs = [item[1] for item in batch]\n",
    "    dec_targets = [item[2] for item in batch]\n",
    "    \n",
    "    # Use torch.nn.utils.rnn.pad_sequence for consistent padding\n",
    "    enc_inputs = nn.utils.rnn.pad_sequence(enc_inputs, batch_first=True, padding_value=VOCAB['[PAD]'])\n",
    "    dec_inputs = nn.utils.rnn.pad_sequence(dec_inputs, batch_first=True, padding_value=VOCAB['[PAD]'])\n",
    "    dec_targets = nn.utils.rnn.pad_sequence(dec_targets, batch_first=True, padding_value=VOCAB['[PAD]'])\n",
    "    \n",
    "    # Create padding masks (True means mask, False means keep)\n",
    "    src_padding_mask = (enc_inputs == VOCAB['[PAD]'])\n",
    "    tgt_padding_mask = (dec_inputs == VOCAB['[PAD]'])\n",
    "    \n",
    "    return enc_inputs, dec_inputs, dec_targets, src_padding_mask, tgt_padding_mask\n",
    "\n",
    "# Generate and split data\n",
    "full_data = generate_synthetic_data(num_samples=5000)\n",
    "train_split = int(0.8 * 5000)\n",
    "val_split = int(0.1 * 5000)\n",
    "\n",
    "train_data = full_data[:train_split]\n",
    "val_data = full_data[train_split:train_split + val_split]\n",
    "test_data = full_data[train_split + val_split:]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(LookupDataset(train_data), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(LookupDataset(val_data), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(LookupDataset(test_data), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "print(f\"Data ready: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6159da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Scaled Dot-Product Attention Function (Helper) ---\n",
    "# Function that calculates QK^T / sqrt(dk) and applies the mask.\n",
    "# NOTE: Positional Encoding (PE) is deliberately omitted in the final Encoder!\n",
    "\n",
    "# --- 2. Multi-Head Attention (MHA) Class ---\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # Initializes Q, K, V projections and implements the core logic:\n",
    "    # Split heads, calculate attention scores, concatenate, final linear layer.\n",
    "    pass\n",
    "\n",
    "# --- 3. Feed-Forward Network (FFN) Class ---\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    # Implements the two-layer linear transformation (Linear -> ReLU -> Linear).\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4790a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. The Encoder Layer ---\n",
    "class EncoderLayer(nn.Module):\n",
    "    # Combines Self-Attention (MHA) and FFN.\n",
    "    # CRITICAL STEP: No PE is added here, proving content-addressability.\n",
    "    # Structure: (Input -> MHA -> Add&Norm) -> (Input -> FFN -> Add&Norm)\n",
    "    pass\n",
    "\n",
    "# --- 2. The Decoder Layer ---\n",
    "class DecoderLayer(nn.Module):\n",
    "    # Structure:\n",
    "    # a. Masked Self-Attention (Input -> Masked MHA -> Add&Norm)\n",
    "    # b. CROSS-ATTENTION (Input -> Cross MHA -> Add&Norm) \n",
    "    # c. FFN (Input -> FFN -> Add&Norm)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Full Encoder Module ---\n",
    "class Encoder(nn.Module):\n",
    "    # Stacks 1 or 2 EncoderLayers. Adds the initial token embedding.\n",
    "    pass\n",
    "\n",
    "# --- 2. Full Decoder Module ---\n",
    "class Decoder(nn.Module):\n",
    "    # Stacks 1 or 2 DecoderLayers. Handles the initial token embedding and PE addition.\n",
    "    # NOTE: PE is typically required in the Decoder for temporal/sequential generation.\n",
    "    pass\n",
    "\n",
    "# --- 3. CrossAttentionLookupModel (The Final Model) ---\n",
    "class CrossAttentionLookupModel(nn.Module):\n",
    "    # Initializes Encoder, Decoder, and the final Projection Layer (maps D_MODEL to VOCAB_SIZE).\n",
    "    # Implements the full forward pass: Enc(X) -> Dec(Y, Enc_Output).\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Initialization ---\n",
    "# Instantiate the model, optimizer (Adam), and loss function (CrossEntropyLoss).\n",
    "model = CrossAttentionLookupModel(...)\n",
    "\n",
    "# --- 2. Training Loop ---\n",
    "# Iterates through NUM_EPOCHS:\n",
    "#   - Trains on train_loader.\n",
    "#   - Evaluates on val_loader to track performance.\n",
    "#   - Includes logic to save the best model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5a25ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Inference Function ---\n",
    "def greedy_decode(model, enc_input, max_len):\n",
    "    # Implements token-by-token autoregressive generation for testing.\n",
    "    pass\n",
    "\n",
    "# --- 2. The Scrambling Test Execution ---\n",
    "# Selects a test batch.\n",
    "# Runs inference on the ORIGINAL enc_input.\n",
    "# Creates a SCRAMBLED version of the enc_input.\n",
    "# Runs inference on the SCRAMBLED enc_input.\n",
    "# Prints the results: Input (Original) -> Output; Input (Scrambled) -> Output.\n",
    "# VERIFICATION: The two outputs must be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1dc164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Attention Hook Setup ---\n",
    "# Add a 'hook' function to the Cross-Attention block in the DecoderLayer\n",
    "# to capture and save the attention weight tensor during the forward pass.\n",
    "\n",
    "# --- 2. Extraction Run ---\n",
    "# Run a single batch from the test_loader through the model to trigger the hook.\n",
    "# Save the extracted attention matrix.\n",
    "\n",
    "# --- 3. Visualization ---\n",
    "def plot_cross_attention(attention_matrix, enc_tokens, dec_tokens):\n",
    "    # Uses Matplotlib/Seaborn to plot the attention_matrix as a heatmap.\n",
    "    # Labels the X-axis with the Encoder tokens and the Y-axis with the Decoder tokens.\n",
    "    # CRITICAL: Visually confirms the sharp spike of attention from the query token to the value token.\n",
    "    pass\n",
    "\n",
    "# Execute plot_cross_attention to display the key heatmap."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
